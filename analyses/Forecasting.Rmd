---
title: "Forecasting clicks & conversions from Google AdWords data"
output: html_notebook
---


Let's import the Google Adwords data for inspection:
```{r}
library(readr)
ad_data_daily <- read_csv("D:/Git/RoomSage/data/ad_data_daily.csv", 
    col_types = cols(clicks = col_integer(), 
        conversions = col_integer(), date = col_date(format = "%Y-%m-%d"), 
        impressions = col_integer(), reservations = col_integer(), 
        total_conversion_value = col_double()))
str(ad_data_daily)
```
Let's summarize our data to get the feel of it:
```{r}
summary(ad_data_daily)
```
In the given dataset, we are dealing with 2 years worth of daily aggregates of impressions (views), clicks (engagements), and final purchases (conversions). Each day is further characterized using the cost of running the Ad campaign on that day, with information about the total added value from the conversions. Finally, we are informed about the average position of our Ad in the ranking of competing Ads.

Let's now inspect the correlation structure in our data for any obvious patterns that will inform our further modelling:
```{r}
correlations <- round(cor(ad_data_daily[,-1], method="spearman"), 2) # use Spearman's rank correlation


# rank correlations
reorder.cormat <- function(matrix){ 
  dd <- as.dist((1-matrix)/2) # Use correlation between variables as distance
  hc <- hclust(dd)
  matrix <- matrix[hc$order, hc$order]
}
correlations <- reorder.cormat(correlations)
# Generate the tall format for ggplotting of the lower triangle of the correlation matrix:
library(reshape2)
correlations[lower.tri(correlations)] <- NA
correlations <- melt(correlations, na.rm = TRUE) # remove the NAs to avoid the upper triangle
# Heatmap
library(ggplot2)
ggplot(data = correlations, aes(Var2, Var1, fill = value)) + geom_tile(color = "white") + scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), space = "Lab", name="Spearman's\nCorrelation") + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + coord_fixed() + xlab("") + ylab("") + geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) + theme(axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.major = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.ticks = element_blank())

```
From the plot above, we can see that the price & reservations, as well as conversions and the total value of conversions cluster together as they are highly correlated within their pairs. Therefore, it would be safest to exclude one variable from each pair to avoid problems with fitting potential models (highly autocorrelated variables) and to improve the model sensibility. As the price and reservations are not fully autocorrelated (0.92 Spearman's rho), some independent signal might be extracted by including both variables.
Also, we notice that clicks and conversions are far from being well correlated (0.17 Spearman's rho), which means the campaign is far from determination by mere number of views.
Color cells by P-value

Let's now examine the periodicity of the target variables, and throw in some familiar indicators such as the weekly and monthly moving averages
```{r}
library(forecast)
library(ggplot2)
ad_data_daily$clicks_weekly_MA <-  ma(ad_data_daily$clicks, order=7)
ad_data_daily$clicks_monthly_MA <- ma(ad_data_daily$clicks, order=30)
ad_data_daily$conversions_weekly_MA <-  ma(ad_data_daily$conversions, order=7)
ad_data_daily$conversions_monthly_MA <- ma(ad_data_daily$conversions, order=30)

ggplot() + geom_line(data = ad_data_daily, aes(date, clicks)) + geom_line(data = ad_data_daily, aes(date, clicks_weekly_MA, colour = "Weekly Moving Average")) + geom_line(data = ad_data_daily, aes(date, clicks_monthly_MA, colour = "Monthly Moving Average")) + scale_x_date('month') + theme_bw() + theme(legend.position = "bottom")
```
```{r}
ggplot() + geom_line(data = ad_data_daily, aes(date, conversions)) + geom_line(data = ad_data_daily, aes(date, conversions_weekly_MA, colour = "Weekly Moving Average")) + geom_line(data = ad_data_daily, aes(date, conversions_monthly_MA, colour = "Monthly Moving Average")) + scale_x_date('month') + theme_bw() + theme(legend.position = "bottom")
```
Just as we know that the day of the week might play a significant role in our shopping behaviours, we may also see a characteristic periodicity of the clicks and conversions in our Ad campaign.
Also, clicks and conversions do not seem to deviate greatly from the mean value throughout the year which should improve the robustness of our inference on this dataset.
So far so good. Let's try to decompose the signal into the constituents: overall trend (increasing / decreasing?), seasonality (related to the seasons of the year, i.e. holidays), and the weekly cycle (i.e. weekends generate more clicks and conversions).
While on the topic of weekly cycles, let's test my intuitive hypothesis that weekend (Fri, Sat, Sun) clicks and conversions are higher on average than on weekdays (Mon-Thu). We will use a non-parametric Wilcoxon ran-sum test.
```{r}
ad_data_daily$day <- weekdays(ad_data_daily$date)
# clicks
wilcox.test(x = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$clicks, # a weekend subset
           y = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$clicks) # a weekday subset
# conversions
wilcox.test(x = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$conversions, # a weekend subset
           y = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$conversions) # a weekday subset

lapply(list(mean_clicks_weekends = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$clicks,
            mean_conversions_weekdays = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$clicks), mean)
lapply(list(mean_clicks_weekends = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$conversions,
            mean_conversions_weekdays = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$conversions), mean)
```
From the above, we may conclude that the number of clicks is significantly different (however lower!) between the weekends than on the weekdays, while the conversions do not seem to bear a similar strength of signal as the p-value we observed (0.1639) would typically be a base for rejection of our hypothesis. However, in both cases the conversions and cliks were lower on average on the weekend than on weekdays. One explanation for this finding is that our Ad campaign was competing against a greater number of competing campagins, as we could not afford to bid for the top spots. Let's test this again using the average position metric:
```{r}
wilcox.test(x = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$average_position, # a weekend subset
           y = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$average_position) # a weekday subset
lapply(list(mean_averagePosition_weekends = subset(ad_data_daily, day %in% c("Friday", "Saturday", "Sunday"))$average_position, 
            mean_averagePosition_weekdays = subset(ad_data_daily, !day %in% c("Friday", "Saturday", "Sunday"))$average_position), mean)
```
My hypothesis was rejected again (p-value = 0.7472) so as a novice in marketing field, my intuition seems to fail me. Perhaps some parameters of the Ad campaign were set-up this way.

Let's now actually decompose the signal in the given data using an Autoregressive Integrated Moving Average model (ARIMA), a widely used tool for time series analysis and modelling.

First, we can start off by excluding the seasonality (based off the 30-day MA) in our data to evaluate the trend and residual signal.
```{r}
plot(na.omit(ad_data_daily$clicks_weekly_MA))
plot(na.omit(ad_data_daily$clicks_monthly_MA))
plot(na.omit(ad_data_daily$conversions_weekly_MA))
plot(na.omit(ad_data_daily$conversions_monthly_MA))
```
From the plots above, we can clearly see the seasonality for clicks and conversions, especially when looking at the monthly moving averages. We should account for that in our prognostication model.
Let's now use 
```{r}
plot(stl(ts(na.omit(ad_data_daily$clicks_weekly_MA), frequency = 365/4), s.window="periodic")) # assuming 4 seasons, let's decompose the weekly-smoothed time series into seasonal, trend and irregular components using loess (STL function)
```
```{r}
plot(stl(ts(na.omit(ad_data_daily$conversions_weekly_MA), frequency = 365/4), s.window="periodic")) # assuming 4 seasons, let's decompose the weekly-smoothed time series into seasonal, trend and irregular components using loess (STL function)
```

Correcting for seasonal and trend effects, there is still residual effect which we should be able to model. Worth noting at this point is that the conversions data is comparatively ill-behaved in terms of the trend and seasonality, however. We have seen in the past plots that conversions are rare indeed.
```{r}
summary(factor(ad_data_daily$conversions))
```


```{r}
library(forecast) # forecast library implements ARIMA
fit_clicks <- auto.arima(ad_data_daily$clicks, seasonal=TRUE, lambda = 0) # lambda to bound the model on 0 clicks for the lower limit
summary(forecast(fit_clicks, h=7))
plot(forecast(fit_clicks, h=7), include=50, xlab = "days", ylab = "clicks") # include 50 last days before displaying our predictions for the following 14 consequitive days
```
The model for clicks appears credible, producing click counts in the ballpark of the previous days and weeks.
```{r}
fit_conversions <- auto.arima(ad_data_daily$conversions, seasonal=TRUE, lambda = 0) 
```
Attempting this task, we could not identify a suitable ARIMA model that captures the seasonality of conversions and subsequently models them in time. This is not surprising, as the number of conversions is typically very low (median = 0, mean = 0.2709, mode on successful days = 1). This also means that the time series is not very informative in terms of predicting the conversions.
This could be addressed by modelling the probability / expected number of conversions given the clicks that we see / predict, in aggregate - regardless of the time series.
In order to do so, we could fit a Poisson model that considers the number of conversions conditional on the success rate (clicks that produce revenue). For that, a linear regression model with Poisson link function could be quickly fit as follows:
```{r}
ad_data_daily$success_rate <- ad_data_daily$conversions / ad_data_daily$clicks
poisson_conversions <- glm(conversions ~ clicks, data = ad_data_daily, family="poisson")
summary(poisson_conversions)
```
It's not a great model, but the correlation of clicks and conversions is not good neither (reminder: Spearman's rho = 0.18).
Now that we have a predictive model for conversions given the clicks, we may begin predicting the conversion rates for the next 14 days of our campaign, given the point forecasts of clicks:
```{r}
conversions_predicted <- predict(poisson_conversions, newdata = data.frame(clicks = forecast(fit_clicks, h=7)$mean), type="response") # predicts the probability of a response
scaling_coefficient <- mean(ad_data_daily$success_rate) / mean(predict(poisson_conversions, newdata = data.frame(clicks = ad_data_daily$clicks), type="response")) # calculate the probability scaling coefficient to account for the model not predicting the mean success rate across the dataset
conversions_predicted <- conversions_predicted * scaling_coefficient
conversions_predicted
```
Let's now see how many conversions we should expect given these probabilities.
```{r}
mean(sapply(1:10000, FUN = function(x) sum(rpois(n=7, lambda = conversions_predicted)))) # sampling 7 random deviates given the predicted probabilities of a response 10000 times, and observing the average
```
Which not surprisingly corresponds to the sum of predicted probabilities:
```{r}
sum(conversions_predicted)
```
Let's now plot the conversion rates and our predictions:
```{r}
plotter <- ad_data_daily[632:731, c("date", "success_rate")]
plotter$type <- "observed"

plotter <- rbind(plotter, data.frame(date = ad_data_daily[725:731, "date"] + 7:13, 
success_rate = conversions_predicted, type = rep("prognosticated", 7)), # generating next 7 days
data.frame(date = ad_data_daily$date, success_rate = ma(ad_data_daily$success_rate, order = 30), type = rep("30-day moving average", nrow(ad_data_daily)))[632:731,], # adding the 30-day moving average
data.frame(date = ad_data_daily$date, success_rate = predict(poisson_conversions, newdata = data.frame(clicks = ad_data_daily$clicks), type="response") * scaling_coefficient, type = rep("predicted", nrow(ad_data_daily)))[632:731,]) # adding the predictions on observed days
ggplot(plotter, aes(x=date, y=log10(success_rate), colour=type)) + geom_line() + theme_bw() + theme(legend.position = "bottom")
```
The model looks quite decent, judging its predictions by the deviation from the trend and by continuity with the prognosis.
Calculating the r-squared between the moving average and our model:
```{r}
library(tidyr)
last_100_days <- spread(data = plotter, key = type, value = success_rate)[1:100,] # generate the wide data format for convenient vector calculations
rsq <- function (x, y) cor(x, y) ^ 2 # r-squared function
rsq(last_100_days[1:85, "30-day moving average"], last_100_days[1:85, "predicted"])
rsq(last_100_days[, "observed"], last_100_days[, "predicted"])
```
So the r-squared between the predictions and the 85 last days for which we could determine the 30-day moving average is not that bad after all. As the observed conversions are quite rare, we did not expect decent r-squared with our predictions.

Perhaps a simpler model using just the simple moving average would do the job just fine?
```{r}
require(smooth)
sma <- sma(ad_data_daily$success_rate, h=365/4)
sma
```

```{r}
rsq(ad_data_daily$success_rate[632:731], forecast(sma, h=731)$mean[632:731])
```
```{r}
rsq(last_100_days[, "predicted"], forecast(sma, h=731)$mean[632:731])
```

```{r}
sessionInfo()
```

